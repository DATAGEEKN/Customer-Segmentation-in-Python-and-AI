{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCp/MZ0ZBncMOHxved3Jpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DATAGEEKN/Customer-Segmentation-in-Python-and-AI/blob/main/Customer_Segmentation_in_Python_and_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Google AI library\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# For Colab-specific features\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# For ML/Clustering\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To display dataframes nicely side-by-side\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "YSroRifTN-RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the API key and configure the Gemini API\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"‚úÖ Gemini API configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not configure Gemini API. Please check your secret settings. Error: {e}\")"
      ],
      "metadata": {
        "id": "WRLzGZh-O8_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DIAGNOSTIC CELL: Check for available columns ---\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mdoganozgun/product-enrichment-agent/refs/heads/main/data/enriched_retail.csv'\n",
        "\n",
        "try:\n",
        "    df_check = pd.read_csv(url)\n",
        "    print(\"‚úÖ File loaded for inspection. The available columns are:\")\n",
        "    print(list(df_check.columns))\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not load file for diagnosis. Error: {e}\")"
      ],
      "metadata": {
        "id": "qqPSGg0GQ3KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 - UPDATED to use all available rich columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# URL for the single, pre-enriched retail dataset\n",
        "url = 'https://raw.githubusercontent.com/mdoganozgun/product-enrichment-agent/refs/heads/main/data/enriched_retail.csv'\n",
        "\n",
        "print(f\"‚¨áÔ∏è Loading the fully enriched dataset from:\\n{url}\")\n",
        "\n",
        "try:\n",
        "    df_enriched = pd.read_csv(url)\n",
        "\n",
        "    # --- Data Cleaning (Updated) ---\n",
        "    # Define all the columns we expect to use.\n",
        "    required_columns = [\n",
        "        'Description', 'CustomerID', 'category', 'usage_context',\n",
        "        'price_segment', 'material_type', 'target_gender', 'target_age_group', 'tags'\n",
        "    ]\n",
        "\n",
        "    # Drop rows where any of our key enrichment columns are missing\n",
        "    df_enriched.dropna(subset=required_columns, inplace=True)\n",
        "\n",
        "    if 'InvoiceNo' in df_enriched.columns:\n",
        "        df_enriched = df_enriched[~df_enriched['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "    df_enriched['CustomerID'] = df_enriched['CustomerID'].astype(int)\n",
        "\n",
        "    print(\"\\n‚úÖ Fully enriched data loaded and cleaned successfully!\")\n",
        "    print(f\"   Data has {df_enriched.shape[0]} rows and {df_enriched.shape[1]} columns.\")\n",
        "    print(\"\\nSample of the loaded data:\")\n",
        "    display(df_enriched.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå A critical error occurred while loading the data.\")\n",
        "    print(f\"   Error details: {e}\")"
      ],
      "metadata": {
        "id": "82aZV6ODRDbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 - UPDATED to build richer profiles\n",
        "\n",
        "print(\"üõ†Ô∏è Building richer customer profiles using all available features...\")\n",
        "\n",
        "# --- Corrected: Combine all the useful enriched columns into a single string ---\n",
        "feature_columns = [\n",
        "    'category', 'usage_context', 'price_segment',\n",
        "    'material_type', 'target_gender', 'target_age_group', 'tags'\n",
        "]\n",
        "\n",
        "# Convert all feature columns to string and join them\n",
        "df_enriched['all_tags'] = df_enriched[feature_columns].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "\n",
        "# --- For each customer, create a single document of all tags from all their purchases ---\n",
        "customer_profiles = df_enriched.groupby('CustomerID')['all_tags'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# --- Vectorize the Profiles ---\n",
        "# We can now potentially capture more features\n",
        "vectorizer = CountVectorizer(min_df=0.05, max_df=0.9)\n",
        "customer_vectors = vectorizer.fit_transform(customer_profiles['all_tags'])\n",
        "vector_df = pd.DataFrame(customer_vectors.toarray(), columns=vectorizer.get_feature_names_out(), index=customer_profiles['CustomerID'])\n",
        "\n",
        "print(\"‚úÖ Richer customer profile vectors created.\")\n",
        "print(\"\\nSample of Numerical Customer Profile Vectors:\")\n",
        "display(vector_df.head())\n"
      ],
      "metadata": {
        "id": "9iNs_B1ERG4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 - Cluster Customers into Segments\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Clustering with K-Means ---\n",
        "N_CLUSTERS = 4 # You can change this number to find more or fewer segments\n",
        "\n",
        "print(f\"‚öôÔ∏è  Grouping customers into {N_CLUSTERS} segments using K-Means clustering...\")\n",
        "\n",
        "# It's good practice to scale the data before clustering\n",
        "scaler = StandardScaler()\n",
        "scaled_vectors = scaler.fit_transform(vector_df)\n",
        "\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
        "vector_df['cluster'] = kmeans.fit_predict(scaled_vectors)\n",
        "\n",
        "print(f\"\\n‚úÖ Customers clustered successfully.\")\n",
        "print(\"\\nDistribution of customers across segments:\")\n",
        "\n",
        "# Display the number of customers in each segment\n",
        "display(vector_df['cluster'].value_counts().sort_index().to_frame())"
      ],
      "metadata": {
        "id": "9KVYC-NGROBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6 - AI-Assisted Persona Generation\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import json\n",
        "\n",
        "def create_cluster_persona(cluster_id, top_tags, model):\n",
        "    \"\"\"\n",
        "    The 'Persona Generation Agent'. It takes a cluster's top purchase\n",
        "    tags and creates a descriptive persona using the Gemini AI.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    As an expert marketing strategist for an online retail store, create a detailed customer persona for a segment that primarily buys products related to these keywords: {', '.join(top_tags)}.\n",
        "\n",
        "    Please provide a response as a valid JSON object with the following keys:\n",
        "    - \"persona_name\": A catchy, descriptive name for this segment (e.g., \"The Thoughtful Gifter\", \"The Home Comfort Creator\").\n",
        "    - \"description\": A 1-2 sentence summary of who this person is, their motivations, and what they likely value.\n",
        "    - \"marketing_strategies\": A bulleted list of 3 actionable marketing strategies to effectively engage this specific segment.\n",
        "\n",
        "    JSON Response:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return json.loads(response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating a persona for cluster {cluster_id}: {e}\")\n",
        "        return {\"persona_name\": \"Error\", \"description\": \"Failed to generate persona.\", \"marketing_strategies\": []}\n",
        "\n",
        "# --- Analyze and Interpret Each Cluster ---\n",
        "all_personas = []\n",
        "# Ensure the Gemini model is initialized\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "print(\"ü§ñ Deploying AI Persona Generation Agent...\")\n",
        "\n",
        "for cluster_id in range(N_CLUSTERS):\n",
        "    print(f\"\\nAnalyzing Cluster {cluster_id}...\")\n",
        "\n",
        "    # Isolate the customers belonging to this cluster\n",
        "    cluster_customers = vector_df[vector_df['cluster'] == cluster_id].index\n",
        "\n",
        "    # Find the top 10 most common features for this cluster\n",
        "    # We drop the 'cluster' column before summing up the features\n",
        "    top_tags = vector_df.loc[cluster_customers].drop('cluster', axis=1).sum().sort_values(ascending=False).head(10).index.tolist()\n",
        "    print(f\"  > Top keywords for this group: {top_tags}\")\n",
        "\n",
        "    # Call the AI agent to create a persona\n",
        "    print(f\"  > Sending data to Gemini to generate persona...\")\n",
        "    persona_data = create_cluster_persona(cluster_id, top_tags, gemini_model)\n",
        "    persona_data['cluster_id'] = cluster_id\n",
        "    persona_data['top_keywords'] = ', '.join(top_tags)\n",
        "    all_personas.append(persona_data)\n",
        "\n",
        "print(\"\\n\\n--- ‚≠êÔ∏è FINAL CUSTOMER SEGMENT PERSONAS ‚≠êÔ∏è ---\")\n",
        "personas_df = pd.DataFrame(all_personas)[['cluster_id', 'persona_name', 'description', 'top_keywords', 'marketing_strategies']]\n",
        "\n",
        "# Prettify the display for easier reading\n",
        "personas_df['marketing_strategies'] = personas_df['marketing_strategies'].apply(\n",
        "    lambda x: \"‚Ä¢ \" + \"\\n‚Ä¢ \".join(x) if isinstance(x, list) else \"N/A\"\n",
        ")\n",
        "display(HTML(personas_df.to_html(index=False, justify='left')))"
      ],
      "metadata": {
        "id": "A0IbxmnHRgyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}